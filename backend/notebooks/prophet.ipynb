{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEW! : ML Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from category_encoders import TargetEncoder  # target encoding for categorical features\n",
    "from statsmodels.tsa.stattools import adfuller  # differencing\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"final_mock_data_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_pipeline(df, numerical_cols, period, target_var, n_pred_periods=4):\n",
    "    \"\"\"Takes in a df, prediction period, output predictions\"\"\"\n",
    "\n",
    "    def encode_categorical_resample(df, period, target_var):\n",
    "        \"\"\"\n",
    "        - Removes ID columns, low-correlated columns to target var\n",
    "        - Target encodes categorical variables\n",
    "        - Resamples df to user required period\n",
    "\n",
    "        Output: df with useful columns (categorical encoded & numerical)\n",
    "        \"\"\"\n",
    "        # First handle string columns\n",
    "        string_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "        for col in string_cols:\n",
    "            if df[col].nunique() > 0.5 * len(\n",
    "                df\n",
    "            ):  # concept: if you got a shit ton of unique values, it's probably an ID\n",
    "                df = df.drop(columns=[col])\n",
    "\n",
    "        # Then handle numerical columns correlation\n",
    "        numerical_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "        correlation_threshold = (\n",
    "            0.4  # Higher as multicollinearity and feature relevance are major concerns.\n",
    "        )\n",
    "        corr_matrix = df[numerical_cols].corr()\n",
    "        cols_to_drop = [\n",
    "            col\n",
    "            for col in corr_matrix.columns\n",
    "            if (corr_matrix[col].abs() < correlation_threshold).sum()\n",
    "        ]\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "        # Initialize target encoder\n",
    "        target_encoder = TargetEncoder()\n",
    "\n",
    "        # Apply target encoding to categorical columns\n",
    "        categorical_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "        df[categorical_cols] = target_encoder.fit_transform(\n",
    "            df[categorical_cols], df[target_var]\n",
    "        )\n",
    "\n",
    "        # apply groupby to get the mean/ mode of the target by each group\n",
    "        try:\n",
    "            df = df.resample(\n",
    "                period\n",
    "            ).agg(\n",
    "                {\n",
    "                    col: \"mean\"\n",
    "                    if col in categorical_cols\n",
    "                    else \"sum\"  # mean vector for features originally categorical, sum for numerical\n",
    "                    for col in df.columns\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Check if too many rows became nil after resampling\n",
    "            nil_ratio = df.isna().sum() / len(df)\n",
    "            if (nil_ratio > 0.5).any():  # If more than 50% of any column is nil\n",
    "                raise ValueError(\n",
    "                    f\"Resampling period '{period}' is too granular for the input data frequency. Many rows became nil after resampling, select another PERIOD\"\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error during resampling: {str(e)}\")\n",
    "\n",
    "        # Drop columns with a single unique value or constant\n",
    "        df = df.loc[\n",
    "            :, df.nunique() > 1\n",
    "        ]  # logic -- if categorical vector is same for all periods, it doesn't matter\n",
    "\n",
    "        return df\n",
    "\n",
    "    def feature_engineering(df, numerical_cols, target_var):\n",
    "        \"\"\"\n",
    "        Manually implement feature engineering on the input DataFrame,\n",
    "\n",
    "        Perform feature engineering on the input DataFrame, implementing:\n",
    "        - Log transform on money related fields -- (revenue, ad spend)\n",
    "        - EWA (Exponential Weighted Average)\n",
    "        - Moving average\n",
    "        - Auto differencing (for stationarity)\n",
    "        - Lagged variables (for time series)\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame with additional engineered features as new columns\n",
    "        \"\"\"\n",
    "\n",
    "        dropped_col = df[target_var].copy()\n",
    "        df = df.drop(columns=[target_var])\n",
    "\n",
    "        for col in numerical_cols:\n",
    "            if col != target_var:\n",
    "                df[f\"log_{col}\"] = np.log(df[col])  # Log transform\n",
    "                df[f\"ewa_{col}\"] = (\n",
    "                    df[col].ewm(span=12, adjust=False).mean().shift(1)\n",
    "                )  # Exponential Weighted Average\n",
    "                df[f\"ma_{col}\"] = (\n",
    "                    df[col].rolling(window=12).mean().shift(1)\n",
    "                )  # Moving Average\n",
    "\n",
    "        def check_stationarity(series):\n",
    "            result = adfuller(series.dropna())\n",
    "            return result[1] < 0.05  # Returns True if stationary\n",
    "\n",
    "        # Check and apply differencing for each feature (max 2 times)\n",
    "        diffed_columns = []\n",
    "        for col in df.columns:\n",
    "            diff_count = 1\n",
    "            while (\n",
    "                not check_stationarity(df[col]) and diff_count < 3\n",
    "            ):  # avoid over-differencing -> white noise\n",
    "                df[col] = df[col].diff()\n",
    "                df[f\"{col}_diff{diff_count}\"] = df[col]\n",
    "                diff_count += 1\n",
    "            if diff_count > 1:  # If differencing was applied\n",
    "                diffed_columns.append(col)\n",
    "                ## Special check for TARGET variable\n",
    "                # if col == TARGET:\n",
    "                #     TARGET = f'{col}_diff{diff_count-1}'  # Update TARGET with the last diffed version\n",
    "\n",
    "        # Remove original columns that had differencing applied -- good practice\n",
    "        df.drop(columns=diffed_columns, inplace=True)\n",
    "\n",
    "        # Create lagged variables for each column, except for the target\n",
    "        for col in df.columns:\n",
    "            df[f\"{col}_lag1\"] = df[col].shift(1)\n",
    "            df[f\"{col}_lag2\"] = df[col].shift(2)\n",
    "\n",
    "        df[target_var] = dropped_col\n",
    "        df = (\n",
    "            df.dropna()\n",
    "        )  ## Drop rows with NaN values, better to use for moving averages\n",
    "\n",
    "        return df\n",
    "\n",
    "    def feature_selection(\n",
    "        df, target_var, vif_threshold=7, xgb_importance_threshold=0.05\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Hybrid feature selection combining VIF and XGBoost importance.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame of features\n",
    "            vif_threshold: Maximum allowed VIF value (default=7)\n",
    "            xgb_importance_threshold: Minimum feature importance threshold (default=0.01)\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with selected features (columns)\n",
    "        \"\"\"\n",
    "        # First pass: VIF-based selection\n",
    "        dropped_col = df[target_var].copy()\n",
    "        df = df.drop(columns=[target_var])\n",
    "        features = df.columns.tolist()\n",
    "\n",
    "        while len(features) > 1:\n",
    "            vif_data = pd.DataFrame()\n",
    "            vif_data[\"Feature\"] = features\n",
    "            vif_data[\"VIF\"] = [\n",
    "                variance_inflation_factor(df[features].values, i)\n",
    "                for i in range(len(features))\n",
    "            ]\n",
    "\n",
    "            if vif_data[\"VIF\"].max() <= vif_threshold:\n",
    "                print(f\"\\nAll VIF values are below {vif_threshold}\")\n",
    "                break\n",
    "\n",
    "            highest_vif_feature = vif_data.loc[vif_data[\"VIF\"].idxmax(), \"Feature\"]\n",
    "            features.remove(highest_vif_feature)\n",
    "            # print(f\"\\nRemoving feature with highest VIF: {highest_vif_feature}\")\n",
    "\n",
    "            if len(features) == 1:\n",
    "                print(\"\\nOnly one feature remaining\")\n",
    "                break\n",
    "\n",
    "        print(f\"\\nRemaining features after VIF selection: {len(features)}\")\n",
    "        print(\"Selected features:\", features)\n",
    "\n",
    "        df = df[features]\n",
    "        df[target_var] = dropped_col\n",
    "\n",
    "        return df\n",
    "\n",
    "    def prophet_forecast(df, target_var, period, n_pred_periods):\n",
    "        df = df.reset_index()\n",
    "        # Prepare data for Prophet\n",
    "        df_prophet = df.rename(columns={\"Date\": \"ds\", target_var: \"y\"})\n",
    "\n",
    "        # Initialize Prophet model\n",
    "        model = Prophet()\n",
    "        model.fit(df_prophet)\n",
    "\n",
    "        # Create future dataframe for prediction\n",
    "        future = model.make_future_dataframe(periods=n_pred_periods, freq=period)  #\n",
    "\n",
    "        # Make predictions\n",
    "        forecast = model.predict(future)\n",
    "        forecast_snippet = forecast[[\"ds\", \"yhat\"]][-n_pred_periods:]\n",
    "\n",
    "        return forecast_snippet, model  # can export model (future saving/ testing)\n",
    "\n",
    "    df = encode_categorical_resample(df, period, target_var)\n",
    "    df = feature_engineering(df, numerical_cols, target_var)\n",
    "    df = feature_selection(df, target_var)\n",
    "    df, model = prophet_forecast(df, target_var, period, n_pred_periods)\n",
    "\n",
    "    return df, model\n",
    "\n",
    "\n",
    "df2, acct_model = prediction_pipeline(df, numerical_cols, \"M\", \"new_accounts\", 4)\n",
    "df2, rev_model = prediction_pipeline(df, numerical_cols, \"M\", \"revenue\", 4)\n",
    "df2, adspend_model = prediction_pipeline(df, numerical_cols, \"M\", \"ad_spend\", 4)\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walk forward validation to test Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_validation(\n",
    "    df, model, target_var, initial_train_size=12, test_window=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform walk-forward validation on a Prophet model to evaluate its predictive accuracy\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame containing the time series data\n",
    "        model: Pre-trained Prophet model\n",
    "        target_var: Name of the target variable column\n",
    "        initial_train_size: Initial number of periods to use for training\n",
    "        test_window: Number of periods to predict in each iteration\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics and prediction results\n",
    "    \"\"\"\n",
    "    # Initialize lists to store results\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "    dates = []\n",
    "    rmse_list = []\n",
    "    mape_list = []\n",
    "\n",
    "    # Prepare data in Prophet format\n",
    "    df_prophet = df.copy()\n",
    "    df_prophet = df_prophet.reset_index()  # Reset index to make Date a column\n",
    "    df_prophet = df_prophet.rename(columns={\"Date\": \"ds\", target_var: \"y\"})\n",
    "\n",
    "    # Calculate total iterations for progress bar\n",
    "    total_iterations = len(df_prophet) - initial_train_size - test_window + 1\n",
    "\n",
    "    # Perform walk-forward validation with progress bar\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    for i in tqdm(\n",
    "        range(initial_train_size, len(df_prophet) - test_window + 1),\n",
    "        desc=f\"Validating {target_var} model\",\n",
    "        total=total_iterations,\n",
    "    ):\n",
    "        # Get test data for current window\n",
    "        test_data = df_prophet.iloc[i : i + test_window]\n",
    "\n",
    "        # Make prediction\n",
    "        future = test_data[[\"ds\"]]\n",
    "        forecast = model.predict(future)\n",
    "        y_pred = forecast[\"yhat\"].values[0]\n",
    "        y_true = test_data[\"y\"].values[0]\n",
    "\n",
    "        # Store results\n",
    "        actuals.append(y_true)\n",
    "        predictions.append(y_pred)\n",
    "        dates.append(test_data[\"ds\"].values[0])\n",
    "\n",
    "        # Calculate error metrics\n",
    "        rmse = np.sqrt(mean_squared_error([y_true], [y_pred]))\n",
    "        rmse_list.append(rmse)\n",
    "\n",
    "        # Calculate MAPE (handle division by zero)\n",
    "        if y_true != 0:\n",
    "            mape = np.abs((y_true - y_pred) / y_true) * 100\n",
    "        else:\n",
    "            mape = np.nan\n",
    "        mape_list.append(mape)\n",
    "\n",
    "    # Calculate summary metrics\n",
    "    avg_rmse = np.mean(rmse_list)\n",
    "    avg_mape = np.nanmean(mape_list)\n",
    "\n",
    "    # Return comprehensive results\n",
    "    return {\n",
    "        \"avg_rmse\": avg_rmse,\n",
    "        \"avg_mape\": avg_mape,\n",
    "        \"actuals\": actuals,\n",
    "        \"predictions\": predictions,\n",
    "        \"dates\": dates,\n",
    "        \"target_var\": target_var,\n",
    "        \"rmse_list\": rmse_list,\n",
    "        \"mape_list\": mape_list,\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate models\n",
    "print(\"Model Validation Results:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Revenue model evaluation\n",
    "rev_results = walk_forward_validation(df, rev_model, \"revenue\")\n",
    "print(\"\\nRevenue Model:\")\n",
    "print(f\"RMSE: {rev_results['avg_rmse']:.2f}\")\n",
    "print(f\"MAPE: {rev_results['avg_mape']:.2f}%\")\n",
    "\n",
    "# # Ad spend model evaluation\n",
    "# ad_results = walk_forward_validation(df, adspend_model, 'ad_spend')\n",
    "# print(f\"\\nAd Spend Model:\")\n",
    "# print(f\"RMSE: {ad_results['avg_rmse']:.2f}\")\n",
    "# print(f\"MAPE: {ad_results['avg_mape']:.2f}%\")\n",
    "\n",
    "# # New accounts model evaluation\n",
    "# acct_results = walk_forward_validation(df, acct_model, 'new_accounts')\n",
    "# print(f\"\\nNew Accounts Model:\")\n",
    "# print(f\"RMSE: {acct_results['avg_rmse']:.2f}\")\n",
    "# print(f\"MAPE: {acct_results['avg_mape']:.2f}%\")\n",
    "\n",
    "# # Plot the results\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create subplots for each metric\n",
    "# fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "# fig.suptitle('Predicted vs Actual Values')\n",
    "\n",
    "# # Revenue plot\n",
    "# axes[0].plot(rev_results['dates'], rev_results['actuals'], label='Actual', marker='o')\n",
    "# axes[0].plot(rev_results['dates'], rev_results['predictions'], label='Predicted', marker='x')\n",
    "# axes[0].set_title('Revenue')\n",
    "# axes[0].legend()\n",
    "# axes[0].grid(True)\n",
    "\n",
    "# # Ad Spend plot\n",
    "# axes[1].plot(ad_results['dates'], ad_results['actuals'], label='Actual', marker='o')\n",
    "# axes[1].plot(ad_results['dates'], ad_results['predictions'], label='Predicted', marker='x')\n",
    "# axes[1].set_title('Ad Spend')\n",
    "# axes[1].legend()\n",
    "# axes[1].grid(True)\n",
    "\n",
    "# # New Accounts plot\n",
    "# axes[2].plot(acct_results['dates'], acct_results['actuals'], label='Actual', marker='o')\n",
    "# axes[2].plot(acct_results['dates'], acct_results['predictions'], label='Predicted', marker='x')\n",
    "# axes[2].set_title('New Accounts')\n",
    "# axes[2].legend()\n",
    "# axes[2].grid(True)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archive (addition to feature selection -- XGBoost hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Second pass: XGBoost-based selection\n",
    "# X = df[features]\n",
    "# y = df[TARGET]\n",
    "\n",
    "# # Train XGBoost model\n",
    "# xgb_model = XGBRegressor(random_state=42)\n",
    "# xgb_model.fit(X, y)\n",
    "\n",
    "# # Get feature importance\n",
    "# importance_df = pd.DataFrame({\n",
    "#     'Feature': features,\n",
    "#     'Importance': xgb_model.feature_importances_\n",
    "# })\n",
    "\n",
    "# print(\"\\nXGBoost Feature Importance:\")\n",
    "# print(importance_df.sort_values('Importance', ascending=False))\n",
    "\n",
    "# # Select features based on importance threshold\n",
    "# selected_features = importance_df[importance_df['Importance'] >= xgb_importance_threshold]['Feature'].tolist()\n",
    "\n",
    "# # selected_features.append(target_col)  # Add back target column (make sure added back properly)\n",
    "\n",
    "# print(f\"\\nSelected {len(selected_features)-1} features based on XGBoost importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_filtered)\n",
    "\n",
    "# # Split Data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_filtered, df['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# # Train XGBoost for Feature Importance\n",
    "# xgb_model = XGBRegressor()\n",
    "# xgb_model.fit(X_train, y_train)\n",
    "# feature_importance = pd.DataFrame({'Feature': X_train.columns, 'Importance': xgb_model.feature_importances_})\n",
    "# top_features = feature_importance.sort_values(by='Importance', ascending=False).head(5)['Feature'].tolist()\n",
    "# X_train, X_test = X_train[top_features], X_test[top_features]\n",
    "\n",
    "# # Train Prophet Model\n",
    "# df_prophet = df[['target']].reset_index().rename(columns={'date': 'ds', 'target': 'y'})\n",
    "# prophet = Prophet()\n",
    "# prophet.fit(df_prophet)\n",
    "# future = prophet.make_future_dataframe(periods=len(y_test))\n",
    "# prophet_forecast = prophet.predict(future)['yhat'].tail(len(y_test)).values\n",
    "\n",
    "# # Train ARIMAX Model\n",
    "# arimax = SARIMAX(y_train, exog=X_train, order=(1,1,1))\n",
    "# arimax_fit = arimax.fit()\n",
    "# arimax_forecast = arimax_fit.forecast(steps=len(y_test), exog=X_test)\n",
    "\n",
    "# # Train XGBoost Model\n",
    "# xgb_forecast = xgb_model.predict(X_test)\n",
    "\n",
    "# # Train LSTM Model\n",
    "# scaler = MinMaxScaler()\n",
    "# y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1,1))\n",
    "# y_test_scaled = scaler.transform(y_test.values.reshape(-1,1))\n",
    "# lstm_model = Sequential([\n",
    "#     LSTM(50, activation='relu', return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
    "#     Dropout(0.2),\n",
    "#     LSTM(50, activation='relu'),\n",
    "#     Dropout(0.2),\n",
    "#     Dense(1)\n",
    "# ])\n",
    "# lstm_model.compile(optimizer='adam', loss='mse')\n",
    "# lstm_model.fit(np.expand_dims(X_train, axis=2), y_train_scaled, epochs=50, batch_size=16, verbose=0)\n",
    "# lstm_forecast = scaler.inverse_transform(lstm_model.predict(np.expand_dims(X_test, axis=2)))\n",
    "\n",
    "# # Stacking Model (Meta-Learner)\n",
    "# stacking_features = np.column_stack([\n",
    "#     prophet_forecast,\n",
    "#     arimax_forecast,\n",
    "#     xgb_forecast,\n",
    "#     lstm_forecast.flatten()\n",
    "# ])\n",
    "# meta_model = LinearRegression()\n",
    "# meta_model.fit(stacking_features, y_test)\n",
    "# stacked_forecast = meta_model.predict(stacking_features)\n",
    "\n",
    "# # Evaluate Models\n",
    "# def evaluate(y_true, y_pred):\n",
    "#     return {\n",
    "#         'MAE': mean_absolute_error(y_true, y_pred),\n",
    "#         'RMSE': np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "#     }\n",
    "\n",
    "# evaluations = {\n",
    "#     'Prophet': evaluate(y_test, prophet_forecast),\n",
    "#     'ARIMAX': evaluate(y_test, arimax_forecast),\n",
    "#     'XGBoost': evaluate(y_test, xgb_forecast),\n",
    "#     'LSTM': evaluate(y_test, lstm_forecast.flatten()),\n",
    "#     'Stacked Model': evaluate(y_test, stacked_forecast)\n",
    "# }\n",
    "\n",
    "# # Select Best Model\n",
    "# best_model = min(evaluations, key=lambda x: evaluations[x]['RMSE'])\n",
    "# print(\"Best Model:\", best_model)\n",
    "# print(\"Evaluation Scores:\", evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archive: Old function to combine into single DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prophet(from_date, to_date):\n",
    "    df = pd.read_csv(\"final_mock_data.csv\")\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    df = df[df[\"Date\"].isin(pd.date_range(start=from_date, end=to_date))]\n",
    "    time_period = 4\n",
    "    # takes in df_grouped, makes df_plotting\n",
    "\n",
    "    df_grouped = (\n",
    "        df.resample(\"M\", on=\"Date\")\n",
    "        .agg(\n",
    "            {\n",
    "                \"ad_spend\": \"sum\",\n",
    "                \"new_accounts\": \"sum\",\n",
    "                \"revenue\": \"sum\",\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    df_plotting = df_grouped[[\"Date\", \"revenue\", \"ad_spend\", \"new_accounts\"]]\n",
    "    df_plotting.rename(columns={\"Date\": \"ds\"}, inplace=True)\n",
    "\n",
    "    def prophet_forecast(df_plotting, df_grouped, predict_col, time_period):\n",
    "        # Prepare data for Prophet\n",
    "        df_prophet = df_grouped.rename(columns={\"Date\": \"ds\", predict_col: \"y\"})\n",
    "        df_grouped.rename(\n",
    "            columns={\"Date\": \"ds\"}, inplace=True\n",
    "        )  # for subsequent merging\n",
    "\n",
    "        # Initialize Prophet model\n",
    "        model = Prophet()\n",
    "        model.fit(df_prophet)\n",
    "\n",
    "        # Create future dataframe for prediction\n",
    "        future = model.make_future_dataframe(periods=time_period, freq=\"M\")\n",
    "\n",
    "        # Make predictions\n",
    "        forecast = model.predict(future)\n",
    "        forecast_snippet = forecast[[\"ds\", \"yhat\"]][-time_period:]\n",
    "\n",
    "        # Add future months to df_plotting\n",
    "        last_date = df_grouped[\"ds\"].max()\n",
    "        future_dates = pd.date_range(\n",
    "            start=last_date + pd.DateOffset(months=1), periods=time_period, freq=\"M\"\n",
    "        )\n",
    "        future_df = pd.DataFrame({\"ds\": future_dates})\n",
    "\n",
    "        # Only add empty values for columns that don't already exist\n",
    "        for col in [\"revenue\", \"ad_spend\", \"new_accounts\"]:\n",
    "            if col not in future_df.columns:\n",
    "                future_df[col] = np.nan\n",
    "\n",
    "        df_plotting = pd.concat([df_plotting, future_df], ignore_index=True)\n",
    "\n",
    "        # Update predictions in df_plotting\n",
    "        for idx, row in forecast_snippet.iterrows():\n",
    "            df_plotting.loc[df_plotting[\"ds\"] == row[\"ds\"], predict_col] = row[\"yhat\"]\n",
    "\n",
    "        return df_plotting\n",
    "\n",
    "    # Get individual forecasts\n",
    "    df_plotting_rev = prophet_forecast(df_plotting, df_grouped, \"revenue\", 4)\n",
    "    df_plotting_ad = prophet_forecast(df_plotting, df_grouped, \"ad_spend\", 4)\n",
    "    df_plotting_accounts = prophet_forecast(df_plotting, df_grouped, \"new_accounts\", 4)\n",
    "\n",
    "    # Combine the forecasts\n",
    "    df_plotting_combined = pd.DataFrame()\n",
    "    df_plotting_combined[\"ds\"] = df_plotting_rev[\"ds\"]\n",
    "    df_plotting_combined[\"revenue\"] = df_plotting_rev[\"revenue\"]\n",
    "    df_plotting_combined[\"ad_spend\"] = df_plotting_ad[\"ad_spend\"]\n",
    "    df_plotting_combined[\"new_accounts\"] = df_plotting_accounts[\"new_accounts\"]\n",
    "\n",
    "    df_plotting = df_plotting_combined\n",
    "\n",
    "    return df_plotting\n",
    "\n",
    "\n",
    "df2 = prophet(\"2022-01-01\", \"2024-09-30\")\n",
    "print(df2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reccelabs-backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
